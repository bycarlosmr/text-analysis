{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One such technique in the field of text mining is Topic Modelling. As the name suggests, it is a process to automatically identify topics present in a text object and to derive hidden patterns exhibited by a text corpus. Thus, assisting better decision making.\n",
    "\n",
    "**Topic Modelling** is different from rule-based text mining approaches that use regular expressions or dictionary based keyword searching techniques. It is an **unsupervised approach** used for finding and observing the bunch of words (called “topics”) in large clusters of texts.\n",
    "\n",
    "Topics can be defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model should result in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”.\n",
    "\n",
    "Topic Models are **very useful for the purpose for document clustering**, organizing large blocks of textual data, information retrieval from unstructured text and feature selection. For Example – New York Times are using topic models to boost their user – article recommendation engines. Various professionals are using topic models for recruitment industries where they aim to extract latent features of job descriptions and map them to right candidates. They are being used to organize large datasets of emails, customer reviews, and user social media profiles.\n",
    "\n",
    "**Approaches for Topic Modelling**\n",
    "\n",
    "There are many approaches for obtaining topics from a text such as \n",
    "\n",
    "- Term Frequency and Inverse Document Frequency\n",
    "- NonNegative Matrix Factorization techniques. \n",
    "- Latent Dirichlet Allocation (LDA) is the most popular topic modeling technique and in this article, we will discuss the same.\n",
    "\n",
    "**How LDA works**\n",
    "\n",
    "LDA assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution. Given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place.\n",
    "\n",
    "LDA is a matrix factorization technique. In vector space, any corpus (collection of documents) can be represented as a document-term matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
